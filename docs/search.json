[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Zuur & Ieno’s 10 steps",
    "section": "",
    "text": "This website in a reproducible exploration of “A protocol for conducting and presenting results of regression-type analyses” by Alain F. Zuur, and Elena N. Ieno.\nThe 10 steps are all first presented in figure 1 of the paper:\n\n\n\nThe 10 steps of regression analyses.\n\n\nThe focus of the 10 steps on on linear modelling of the type GLM, GLMM etc and uses R although it generalizes to other languages.\n\nAccessing data and code for reproducibility\nThe paper provides data and code on Dryad but is not set up for interactive report-style reproducibility with package versioning. To fix that, I used a quarto document with renv to produce this website. In order to reproduce the analysis here, you can clone the repository in RStudio, install the renv package, run renv::restore() and you should be good to go for running the quarto notebook!\nI had to make some changes to the packages used. Let’s load what we will need now. Packages maptools and rgdal are deprecated as far as I can tell, and sp is just not recommendable for forward compatibility in 2024.\n\n# Necessary packages from original code\nlibrary(lattice)  \nlibrary(ggplot2)\nlibrary(ggmap)\nlibrary(lme4)\nlibrary(mgcv)\n\n# Recommended by the authors but outdated\n# library(maptools) =&gt; We will use other packages\n# library(sp) =&gt; We won't need it with sf\n# library(rgdal) =&gt; We won't need it with sf\n\n# Using sf\nlibrary(sf)\n\n# Other additionnal packages to improve upon the provided code\nlibrary(lubridate)\nlibrary(gganimate)\nlibrary(here)\nlibrary(leaflet)\nlibrary(equatiomatic)\n\nIn order to access the data and code, we would ideally use a package like rdryad to do so, but I have been getting nowhere with it. It is probably broken as it is soon to be superseded by the deposits package. If I forget to update this website with the latest deposits API, feel free the file an issue.\nIn the meantime, let’s download the files one by one.\n\n# Create the file urls and destination files names & names\nbase_dryad_url &lt;- \"https://datadryad.org/stash/downloads/file_stream/\"\nfile_url_list &lt;- paste0(base_dryad_url, c(37547:37550))\nfiles_names &lt;- c(\"monkeys.txt\", \"owls.txt\", \"oystercatchers.txt\", \"zurr_iena_2016.R\")\nfiles_paths_list &lt;- paste0(c(rep(\"data/\", 3), \"scripts/\"), files_names)\n\n# If the file exists already, do not download it\nret &lt;- mapply(\\(file_url, file_path) {\n  if (!file.exists(file_path)) download.file(file_url, destfile =  file_path)\n}, file_url_list, files_paths_list)\n\nYou can take a look at the code in the scripts directory, we will be copying code from there into this document. Now, let’s load the data properly before we get anything else done.\n\nOwls &lt;- read.table(here(\"data\", \"owls.txt\"), \n                   header = TRUE,\n                   dec = \".\")\n\n# SiblingNegotiation is too long....use shorter name:\nOwls$NCalls &lt;- Owls$SiblingNegotiation\n\n# Let's look at it\nnames(Owls)\n\n [1] \"Nest\"               \"Xcoord\"             \"Ycoord\"            \n [4] \"FoodTreatment\"      \"SexParent\"          \"ArrivalTime\"       \n [7] \"SiblingNegotiation\" \"BroodSize\"          \"NegPerChick\"       \n[10] \"Date\"               \"Day\"                \"Month\"             \n[13] \"NCalls\"            \n\nstr(Owls)\n\n'data.frame':   599 obs. of  13 variables:\n $ Nest              : chr  \"AutavauxTV\" \"AutavauxTV\" \"AutavauxTV\" \"AutavauxTV\" ...\n $ Xcoord            : int  556216 556216 556216 556216 556216 556216 556216 556216 556216 556216 ...\n $ Ycoord            : int  188756 188756 188756 188756 188756 188756 188756 188756 188756 188756 ...\n $ FoodTreatment     : chr  \"Deprived\" \"Deprived\" \"Deprived\" \"Deprived\" ...\n $ SexParent         : chr  \"Male\" \"Male\" \"Male\" \"Male\" ...\n $ ArrivalTime       : num  22.2 22.5 22.6 22.6 22.6 ...\n $ SiblingNegotiation: int  4 2 2 2 2 18 18 3 3 3 ...\n $ BroodSize         : int  5 5 5 5 5 5 5 5 5 5 ...\n $ NegPerChick       : num  0.8 0.4 0.4 0.4 0.4 3.6 3.6 0.6 0.6 0.6 ...\n $ Date              : chr  \"12/07/97\" \"12/07/97\" \"12/07/97\" \"12/07/97\" ...\n $ Day               : int  12 12 12 12 12 12 12 12 12 12 ...\n $ Month             : int  7 7 7 7 7 7 7 7 7 7 ...\n $ NCalls            : int  4 2 2 2 2 18 18 3 3 3 ...\n\nhead(Owls)\n\n        Nest Xcoord Ycoord FoodTreatment SexParent ArrivalTime\n1 AutavauxTV 556216 188756      Deprived      Male       22.25\n2 AutavauxTV 556216 188756      Deprived      Male       22.53\n3 AutavauxTV 556216 188756      Deprived      Male       22.56\n4 AutavauxTV 556216 188756      Deprived      Male       22.61\n5 AutavauxTV 556216 188756      Deprived      Male       22.65\n6 AutavauxTV 556216 188756      Deprived      Male       22.76\n  SiblingNegotiation BroodSize NegPerChick     Date Day Month NCalls\n1                  4         5         0.8 12/07/97  12     7      4\n2                  2         5         0.4 12/07/97  12     7      2\n3                  2         5         0.4 12/07/97  12     7      2\n4                  2         5         0.4 12/07/97  12     7      2\n5                  2         5         0.4 12/07/97  12     7      2\n6                 18         5         3.6 12/07/97  12     7     18\n\n\n\nOC &lt;- read.table(here(\"data\", \"oystercatchers.txt\"), \n                 header = TRUE,\n                 dec = \".\")\n\n# Let's look at it\nnames(OC)\n\n[1] \"ShellLength\" \"Month\"       \"FeedingType\" \"FeedingPlot\"\n\nstr(OC)\n\n'data.frame':   197 obs. of  4 variables:\n $ ShellLength: num  1.9 2.16 2.17 2.34 2.2 2.2 1.92 2.11 2.17 2.41 ...\n $ Month      : chr  \"Dec\" \"Dec\" \"Dec\" \"Dec\" ...\n $ FeedingType: chr  \"Hammerers\" \"Hammerers\" \"Stabbers\" \"Hammerers\" ...\n $ FeedingPlot: chr  \"B\" \"B\" \"B\" \"B\" ...\n\nhead(OC)\n\n  ShellLength Month FeedingType FeedingPlot\n1        1.90   Dec   Hammerers           B\n2        2.16   Dec   Hammerers           B\n3        2.17   Dec    Stabbers           B\n4        2.34   Dec   Hammerers           B\n5        2.20   Dec    Stabbers           B\n6        2.20   Dec   Hammerers           B\n\n\n\nMonkeys &lt;- read.table(here(\"data\", \"monkeys.txt\"), \n                      header = TRUE)\n\n# Let's look at it\nnames(Monkeys)\n\n [1] \"SubordinateGrooms\" \"DominantGrooms\"    \"RankDifference\"   \n [4] \"Relatedness\"       \"GroomSymmetry\"     \"Time\"             \n [7] \"FocalHour\"         \"FocalGroomer\"      \"Receiver\"         \n[10] \"GroupSize\"        \n\nstr(Monkeys)\n\n'data.frame':   1674 obs. of  10 variables:\n $ SubordinateGrooms: chr  \"yes\" \"yes\" \"yes\" \"yes\" ...\n $ DominantGrooms   : chr  \"no\" \"yes\" \"yes\" \"no\" ...\n $ RankDifference   : num  0.697 0.632 0.169 0.378 0.511 ...\n $ Relatedness      : num  0.224 0.682 0.707 0.291 0.453 ...\n $ GroomSymmetry    : int  1 1 0 1 1 1 0 1 1 1 ...\n $ Time             : num  4.19 5.6 6.16 6.98 2.85 ...\n $ FocalHour        : int  71 72 72 73 74 75 75 75 75 75 ...\n $ FocalGroomer     : chr  \"ade\" \"vic\" \"vic\" \"pre\" ...\n $ Receiver         : chr  \"ban\" \"yao\" \"pre\" \"nai\" ...\n $ GroupSize        : chr  \"large\" \"large\" \"large\" \"large\" ...\n\nhead(Monkeys)\n\n  SubordinateGrooms DominantGrooms RankDifference Relatedness GroomSymmetry\n1               yes             no      0.6969321   0.2238303             1\n2               yes            yes      0.6324555   0.6823489             1\n3               yes            yes      0.1690309   0.7071068             0\n4               yes             no      0.3779645   0.2913760             1\n5               yes            yes      0.5107539   0.4529901             1\n6               yes            yes      0.2948839   0.4539824             1\n      Time FocalHour FocalGroomer Receiver GroupSize\n1 4.186111        71          ade      ban     large\n2 5.601667        72          vic      yao     large\n3 6.164167        72          vic      pre     large\n4 6.976111        73          pre      nai     large\n5 2.850556        74          dys      ver     small\n6 8.605278        75          pox      ecz     small\n\n\n\n\nStep 1: State appropriate questions\nThe key idea is to have the salient question of the analysis in mind at the start, and formulate them properly. The example here is a study on the “vocal behavior of barn owl siblings” with a N = 28. The hypothesis is that food availability will influence “sibling negotiation”, proxied by the number of calls in the nest, sampled with microphones. Half the nests get extra food (treatment: satiated) and the other half is starved (treatment: deprived ; surprisingly no control?).\nThe 3 covariates are time, food treatment (satiated or deprived) and sex of parent. The question is :\n\nDoes the relationship between sibling negotiation and sex of the parent differ with food treatment, and does the effect of time on sibling negotiation differ with food treatment?\n\nNote that the question contains the 3 terms and expected interactions.\nThe authors warn against breaking down the questions into smaller questions such as “Is there an effect of sex of the parent?”, as “A potential problem with this approach is that the residuals of one model may show patterns when compared with the covariates not used in that model, which invalidates the model assumptions.”\nI find this a little surprising because I was taught to build simple models before complex ones, and would naturally work with simpler single covariate models first.\n\n\nStep 2: Visualize the experimental design\nThis step is simple yet sometimes overlooked: visualize the sampling protocol and experimental design preferably with the help of a map.\nBelow we use the sf package to parse the data as spatial data.\n\n# Parse the dataframe as a sf object with the proper projection, and reproject as\n# WGS 84 CRS (LAT / LONG)\nOwls_sf &lt;- st_as_sf(Owls, coords = c(\"Xcoord\", \"Ycoord\"))\n\nWGS84     &lt;- st_crs(\"+proj=longlat +datum=WGS84\")\nprojSWISS &lt;- st_crs(\"+init=epsg:21781\")\n\nWarning in CPL_crs_from_input(x): GDAL Message 1: +init=epsg:XXXX syntax is\ndeprecated. It might return a CRS with a non-EPSG compliant axis order.\n\nst_crs(Owls_sf) &lt;- st_crs(projSWISS)\n\nOwls_sf_tmp &lt;- st_transform(Owls_sf, WGS84)\nOwls_sf_wgs84 &lt;- cbind(Owls_sf_tmp, st_coordinates(Owls_sf_tmp))\n\n# Let's look at it\nhead(Owls_sf_wgs84)\n\nSimple feature collection with 6 features and 13 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 6.864574 ymin: 46.84849 xmax: 6.864574 ymax: 46.84849\nGeodetic CRS:  +proj=longlat +datum=WGS84\n        Nest FoodTreatment SexParent ArrivalTime SiblingNegotiation BroodSize\n1 AutavauxTV      Deprived      Male       22.25                  4         5\n2 AutavauxTV      Deprived      Male       22.53                  2         5\n3 AutavauxTV      Deprived      Male       22.56                  2         5\n4 AutavauxTV      Deprived      Male       22.61                  2         5\n5 AutavauxTV      Deprived      Male       22.65                  2         5\n6 AutavauxTV      Deprived      Male       22.76                 18         5\n  NegPerChick     Date Day Month NCalls        X        Y\n1         0.8 12/07/97  12     7      4 6.864574 46.84849\n2         0.4 12/07/97  12     7      2 6.864574 46.84849\n3         0.4 12/07/97  12     7      2 6.864574 46.84849\n4         0.4 12/07/97  12     7      2 6.864574 46.84849\n5         0.4 12/07/97  12     7      2 6.864574 46.84849\n6         3.6 12/07/97  12     7     18 6.864574 46.84849\n                   geometry\n1 POINT (6.864574 46.84849)\n2 POINT (6.864574 46.84849)\n3 POINT (6.864574 46.84849)\n4 POINT (6.864574 46.84849)\n5 POINT (6.864574 46.84849)\n6 POINT (6.864574 46.84849)\n\n\nThe code suggests to write the points as a .kml file to open in Google Earth.\n\n# Write the points as kml, wich you can open in google earth\nowls_kml_file &lt;- here(\"data\", \"Owls_wgs84.kml\")\nif (!file.exists(owls_kml_file)) {\n  st_write(Owls_sf_wgs84, \n           owls_kml_file, \n           driver = \"kml\", delete_dsn = TRUE)\n}\n\nThe code relied ggmap, which makes an API request to Google maps. Of course, in 2024 we are required to use an API key for that. As an alternative I suggest leaflet which is more likely to work in the future and does not require to mess with API keys.\n\nleaflet(Owls_sf_wgs84) %&gt;%\n  addTiles() %&gt;%  # Add default OpenStreetMap map tiles\n  addMarkers()\n\n\n\n\n\nFor a more publication-oriented map with ggmap I would recommend using the stadiamap API.\n\n# I suppress the API message\nglgmap &lt;- suppressMessages(get_stadiamap(c(6.7, 46.70, 7.2, 46.96)))\n\np_ggmap &lt;- ggmap(glgmap) + \n  geom_point(aes(X, Y),\n             data = Owls_sf_wgs84,\n             size = 4) + \n  xlab(\"Longitude\") + ylab(\"Latitude\") + \n  theme(text = element_text(size = 15))\np_ggmap\n\n\n\n\nWe can also do a simple sf + ggplot plot.\n\np_simple &lt;- ggplot(Owls_sf_wgs84) + \n  geom_sf(size = 4) + \n  xlab(\"Longitude\") + ylab(\"Latitude\") + \n  theme_light() + theme(text = element_text(size = 15))\np_simple\n\n\n\n\nFinally, the code suggest another two plots that are not in the paper, but are useful. The first time is a plot of the time series.\n\np_series &lt;- ggplot() + \n  xlab(\"Arrival time\") + ylab(\"Number of calls\") + \n  theme(text = element_text(size = 15)) + theme_light() +\n  geom_point(data = Owls_sf_wgs84,\n             aes(x = ArrivalTime,\n                 y = NCalls,\n                 color = FoodTreatment),\n             size = 2) +\n  geom_line(data = Owls_sf_wgs84,\n            aes(x = ArrivalTime,\n                y = NCalls,\n                group = FoodTreatment,\n                color = FoodTreatment)) + \n  facet_wrap( ~ Nest, ncol = 4)\np_series\n\n\n\n\nThe second shows how sampling unfolds across space and time, but I find the plot a little unwieldy as it facets multiple maps.\n\n# We parse the date column as a proper date\nOwls_sf_wgs84$Date_parsed &lt;- as_date(Owls_sf_wgs84$Date, format = \"%d/%m/%y\")\n\np_ggmap_facet &lt;- ggmap(glgmap) + \n  geom_point(aes(X, Y),\n             data = Owls_sf_wgs84,\n             size = 4) + \n  xlab(\"Longitude\") + ylab(\"Latitude\") + \n  theme(text = element_text(size = 15)) + \n  facet_wrap(~Date_parsed) \n\np_ggmap_facet\n\n\n\n\n\n\nStep 3: Conduct data exploration\nThere is another even older Zurr et al. paper for the 10 steps of data exploration. The first figure of that paper gives you the gist of the protocol.\n\n\n\nThe 10 steps for data exploration.\n\n\nIn that paper, the authors warn against “data dredging”, which is when the patterns explored and discovered during data exploration influence data analysis unhealthily. Modeling and testing decisions should be determined a priori, using knowledge of the system and not a posteriori after exploration of the data. When understanding is limited, we can use exploration to help generate hyotheses, but that is fundamentally different from the principled workflow of this paper.\nThen, the authors warn against certain tests and visual tools, including normality tests. The paper is a useful read and could be the next iteration of that project.\nWe now move from owls to oystercatchers. The study related the length of clams preyed upon and the feeding behavior of oystercatchers, accross time and space. The authors describe how the design suggests a 3-way interaction term that would in practicality only be driven by a couple of data points. The following plot shows that in location A in December, there were only two observations, both showing the same value. Note that I modified the plot to add colors to those problematic points to make the visualization clearer\n\n# Set a color column\nOC$color_pt &lt;- ifelse(OC$Month == \"Dec\" & \n                        OC$FeedingType == \"Stabbers\" & \n                          OC$FeedingPlot == \"A\", \n                      \"red\", \"grey\")\n\n# Here is the code for Figure 3\np_OC &lt;- ggplot() + xlab(\"Feeding type\") + ylab(\"Shell length (mm)\") + \n  geom_point(data = OC,\n             aes(x = FeedingType, y = ShellLength),\n             color = OC$color_pt,\n             position = position_jitter(width = .03),\n             size = 2) + \n  facet_grid(Month ~ FeedingPlot,\n             scales = \"fixed\") + \n  theme_light() +\n  theme(text = element_text(size = 15)) + \n  theme(legend.position = \"none\") + \n  theme(strip.text.y = element_text(size = 15,\n                                    colour = \"black\",\n                                    angle = 20),\n        strip.text.x = element_text(size = 15,\n                                    colour = \"black\",\n                                    angle = 0)\n  )\np_OC\n\n\n\n\nYou should spend a good amount of time on data exploration. It will also help you identify data quality issues and encoding errors.\nNote that it is sometimes useful to tabulate data across levels of a given factor.\n\ntable(OC$Month, OC$FeedingPlot, OC$FeedingType)\n\n, ,  = Hammerers\n\n     \n       A  B  C\n  Dec 17 14 26\n  Jan 43 31 34\n\n, ,  = Stabbers\n\n     \n       A  B  C\n  Dec  2  5 15\n  Jan  4  3  3\n\n\n\n\nStep 4: Identify the dependency structure in the data\nThe authors warn that it is rare to find a dataset without dependency structures. They advise GLMM as a way to deal with pseudoreplicated data (relying on pooling as a way to properly model dependencies between, say, sampling locations etc..).\nWe now move on to the baboon dataset. I think it is the same dataset used in Statistical Rethinking for teaching multilevel modelling. The design: the researchers are interested in understanding grooming behavior between baboons. Baboon may hold different ranks within the troop (represented as a value between 0 and 1). Some 60 baboons are sampled multiple times, for an hour at a time, its grooming behavior recorded, making the receiver of the behavior another layer of dependency. This pseudoreplication structure requires a mixed-effects model with random effects (two-way nested AND crossed). The structure looks like this:\n\n\n\nBaboon dataset structure\n\n\nThe authors suggest to properly report the structure. Personally I love seeing, in papers, when both a graphical, a textual AND a mathematical description is given.\nThe textual description for baboon and owl data:\n\nThis data set consists of multiple observations of rank differences of a given baboon and receivers within a focal hour, along with multiple observations of a given receiver. We therefore applied a mixed-effects model with the random effect focal hour nested within the random effect baboon and a crossed random effect receiver\nWe sampled each nest multiple times and therefore applied a GLMM in which nest is used as random intercept, as this models a dependency structure among sibling negotiation observations of the same nest\n\n\n\nStep 5: Present the statistical model\nSpeaking of mathematical description, this step is where the authors suggest this is given and stated clearly, under the form of an equation. Here is what it would look like for the owl data:\n\n\n\nEquation describing the owl data model\n\n\nOne of the key element in this is how the error structure of the data is stated clearly: here the count of calls is modelled as a Poisson distribution. It shows the log link function and how the model is clearly defined. The authors give further advice for how to explain choices of distributions.\nFor the baboon data:\n\n\n\nEquation describing the baboon data model\n\n\nIf only all papers when to that length in describing their models (although I am definitely guilty of omitting details as well). The amazing package equatiomatic makes this easier.\n\n# Although we will look at the model formulation at the next step in greater\n# details, let's try and use it here with equationmatic\n\nM1 &lt;- glmer(NCalls ~ FoodTreatment + ArrivalTime + SexParent +\n                     FoodTreatment : SexParent +\n                     FoodTreatment : ArrivalTime +\n                     (1 | Nest),\n            family = poisson,\n            data = Owls)\nextract_eq(M1, wrap = TRUE, terms_per_line = 1)\n\n\\[\n\\begin{aligned}\n  \\operatorname{NCalls}_{i}  &\\sim \\operatorname{Poisson}(\\lambda_i) \\\\\n    \\log(\\lambda_i) &=\\alpha_{j[i]}\\ + \\\\\n&\\quad \\beta_{1}(\\operatorname{FoodTreatment}_{\\operatorname{Satiated}})\\ + \\\\\n&\\quad \\beta_{2}(\\operatorname{ArrivalTime})\\ + \\\\\n&\\quad \\beta_{3}(\\operatorname{SexParent}_{\\operatorname{Male}})\\ + \\\\\n&\\quad \\beta_{4}(\\operatorname{FoodTreatment}_{\\operatorname{Satiated}} \\times \\operatorname{SexParent}_{\\operatorname{Male}})\\ + \\\\\n&\\quad \\beta_{5}(\\operatorname{ArrivalTime} \\times \\operatorname{FoodTreatment}_{\\operatorname{Satiated}}) \\\\\n    \\alpha_{j}  &\\sim N \\left(\\mu_{\\alpha_{j}}, \\sigma^2_{\\alpha_{j}} \\right)\n    \\text{, for Nest j = 1,} \\dots \\text{,J}\n\\end{aligned}\n\\]\n\n\n\n\nStep 6: Fit the model\nThe authors first remind that it is important to say what software was used to fit the model, directly in the text. They add details as to how to report a MCMC analysis. This section of the paper is not very verbose and could have been expanded to talk about how versions of packages can be very relevant to the fitted results. Also, the paper could have discussed how fitted models are stored and conserved (with rds files) and implications of all this kind of stuff for the ability of future readers to make sense of the model.\nOne note concerning the owl dataset: in the code, the authors mention that the response variable should probably be transformed (but for some reason they do not show the process). They also mention that brood size should probably be used an offset in the model - but did not want to confuse readers of the paper with it, at the risk of confusing readers of the code by not explaining why and how the offset is needed here.\nLet’s see the model again (note the additional code for the offset, if you want, but we won’t use it for the sake of consistency with the paper).\n\n#For the offset we need:\nOwls$LogBroodSize &lt;- log(Owls$BroodSize)\n\nlibrary(lme4)\nM1 &lt;- glmer(NCalls ~ FoodTreatment + ArrivalTime + SexParent +\n                     FoodTreatment : SexParent +\n                     FoodTreatment : ArrivalTime +\n                     #offset(LogBroodSize) +  #Feel free to include\n                     (1 | Nest),\n            family = poisson,\n            data = Owls)\n\n# Numerical results:\nsummary(M1)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: poisson  ( log )\nFormula: \nNCalls ~ FoodTreatment + ArrivalTime + SexParent + FoodTreatment:SexParent +  \n    FoodTreatment:ArrivalTime + (1 | Nest)\n   Data: Owls\n\n     AIC      BIC   logLik deviance df.resid \n  5011.4   5042.1  -2498.7   4997.4      592 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.4559 -1.7563 -0.6413  1.1770 11.1040 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n Nest   (Intercept) 0.235    0.4847  \nNumber of obs: 599, groups:  Nest, 27\n\nFixed effects:\n                                      Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                          5.1698547  0.2926462  17.666   &lt;2e-16 ***\nFoodTreatmentSatiated               -0.6540263  0.4686680  -1.396   0.1629    \nArrivalTime                         -0.1296994  0.0113051 -11.473   &lt;2e-16 ***\nSexParentMale                       -0.0094526  0.0453669  -0.208   0.8349    \nFoodTreatmentSatiated:SexParentMale  0.1297493  0.0704391   1.842   0.0655 .  \nFoodTreatmentSatiated:ArrivalTime   -0.0004913  0.0192171  -0.026   0.9796    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) FdTrtS ArrvlT SxPrnM FTS:SP\nFdTrtmntStt -0.545                            \nArrivalTime -0.938  0.570                     \nSexParentMl -0.060  0.055 -0.038              \nFdTrtmS:SPM  0.033 -0.049  0.024 -0.606       \nFdTrtmnS:AT  0.539 -0.993 -0.573  0.004 -0.043\n\n\n\n\nStep 7: Validate the model\nWhat is validation? “Model validation confirms that the model complies with underlying assumptions.”\nRegression models have assumptions (in particular about the structure of residuals) and the fore violation of those assumptions may lead to increase bias, type 1 error rate, etc. Residuals should be plotted against all variables (and time and space dimensions if applicable) and autocorellation should be modeled if applicable.\nTo get to the residuals and fitted values:\n\nE1 &lt;- resid(M1, type = \"pearson\")\nF1 &lt;- fitted(M1)\nstr(E1)\n\n Named num [1:599] -1.19 -1.87 -1.87 -1.85 -1.85 ...\n - attr(*, \"names\")= chr [1:599] \"1\" \"2\" \"3\" \"4\" ...\n\nstr(F1)\n\n Named num [1:599] 7.18 6.93 6.9 6.85 6.82 ...\n - attr(*, \"names\")= chr [1:599] \"1\" \"2\" \"3\" \"4\" ...\n\n\nIn the context of the owl data, the authors describe finding a pattern of over dispersion by calculating an over dispersion metric:\n\n# Get the dispersion statistic\n# (not counting random effects as parameters)\nE1 &lt;- resid(M1, type = \"pearson\")\nN  &lt;- nrow(Owls)\np  &lt;- length(fixef(M1)) + 1\nsum(E1^2) / (N - p)\n\n[1] 5.439501\n\n\nNot in the paper, they also flag a potential issue with heterogeneity…\n\nplot(x = F1,\n     y = E1,\n     xlab = \"Fitted values\",\n     ylab = \"Pearson residuals\")\nabline(h = 0, lty = 2)\n\n\n\n\nBuilding up to the next figure in the paper, we plot arrival time against those residuals.\n\nplot(x = Owls$ArrivalTime,\n     y = E1,\n     xlab = \"Arrival time (h)\",\n     ylab = \"Pearson residuals\")\nabline(h = 0, lty = 2)\n\n\n\n\nSuspecting a non-linear structure, the authors decide to fit a GAM on the residuals. As they write in the code “If the smoother is not significant, or if it explains a small amount of variation, then there are indeed no residual patterns.”\n\nT2 &lt;- gam(E1 ~ s(ArrivalTime), data = Owls)\nsummary(T2)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nE1 ~ s(ArrivalTime)\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) -0.01448    0.09185  -0.158    0.875\n\nApproximate significance of smooth terms:\n                 edf Ref.df     F  p-value    \ns(ArrivalTime) 7.159  8.206 5.011 4.88e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.0615   Deviance explained = 7.27%\nGCV = 5.1235  Scale est. = 5.0537    n = 599\n\nplot(T2)\nabline(h = 0, lty = 2)\n\n\n\n\nNow, we can reproduce the next figure in the paper. The authors used a plotting technique for the predictions which consists in creating a grid of arrival times to feed to the predict function. To this we add the smoother and its confidence interval, all of it using ggplot2.\n\ntime_range &lt;- range(Owls$ArrivalTime)\nMyData &lt;- data.frame(ArrivalTime = seq(time_range[1], time_range[2], \n                                       length = 100))\nP      &lt;- predict(T2, newdata = MyData, se = TRUE)\nMyData$mu   &lt;- P$fit\nMyData$SeUp &lt;- P$fit + 1.96 * P$se\nMyData$SeLo &lt;- P$fit - 1.96 * P$se\n\nOwls$E1 &lt;- E1\np_M1 &lt;- ggplot() + \n  xlab(\"Arrival time  (h)\") + ylab(\"Pearson residuals\") + \n  theme(text = element_text(size = 15)) + \n  geom_point(data = Owls,\n             aes(x = ArrivalTime, y = E1),\n             size = 1) + \n  geom_line(data = MyData,\n            aes(x = ArrivalTime, y = mu),\n            colour = \"black\") +\n  geom_ribbon(data = MyData,\n              aes(x = ArrivalTime,\n                  ymax = SeUp,\n                  ymin = SeLo ),\n              alpha = 0.2) + \n  geom_hline(yintercept = 0)\np_M1\n\n\n\n\nFrom this exploration of the residuals, the authors conclude that we should be using a GAMM instead…\nA couple of notes: the authors mention in a figure caption that the smooth only explains 7% of the variation in the residuals, which as I understand it (maybe badly), is quite low. They also mention other factors that could be responsible for overdispersion: “a missing covariate, the relatively large number of zero observations (25%) or dependency within (or between) nests”. The pattern with arrival time seems quite clear, and I am left convinced it should be dealt with in the model. I am not left convinced that a complex GAMM would be the first answer, however.\n\n\nStep 8: Interpret and present the numerical output of the model\n\n\nStep 9: Create a visual representation of the model\n\n\nStep 10: Simulate from the model"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This website in an exploration of “A protocol for conducting and presenting results of regression-type analyses” by Alain F. Zuur, and Elena N. Ieno."
  }
]