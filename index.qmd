---
title: "Zuur & Ieno's 10 steps"
---

This website in a reproducible exploration of "[A protocol for conducting and presenting results of regression-type analyses](https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.12577)" by Alain F. Zuur, and Elena N. Ieno.

The 10 steps are all first presented in figure 1 of the paper:

![The 10 steps of regression analyses.](https://besjournals.onlinelibrary.wiley.com/cms/asset/17c3d76c-f2e6-4e2a-a117-581a303cdaef/mee312577-fig-0001-m.png){fig-align="center"}

The focus of the 10 steps on on linear modelling of the type **GLM, GLMM** etc and uses R although it generalizes to other languages.

I found difficult that the authors chose to use different datasets to illustrate different steps. Why not show the entire workflow for one dataset and better demonstrate the entire workflow? Did the authors fear that the use of a single open dataset was limiting for publication?

#### Accessing data and code for reproducibility

The paper provides data and code on Dryad but is not set up for interactive report-style reproducibility with package versioning (see the end of this section for a note about package age). To fix that, I used a quarto document with `renv` to produce this website. In order to reproduce the analysis here, you can clone [the repository](https://github.com/VLucet/ZuurIeno10steps) in [RStudio](https://happygitwithr.com/new-github-first#new-rstudio-project-via-git), install the [`renv`](https://rstudio.github.io/renv/articles/renv.html) package, run `renv::restore()` and you should be good to go for running the quarto notebook!

In order to access the data and code, we would ideally use a package like [`rdryad`](https://github.com/ropensci/rdryad) to do so, but I have been getting nowhere with it. It is probably broken as it is soon to be superseded by the [`deposits`](https://github.com/ropenscilabs/deposits) package. If I forget to update this website with the latest [`deposits` API](https://github.com/ropenscilabs/deposits/issues/41), feel free the [file an issue](https://github.com/VLucet/ZuurIeno10steps/issues).

In the meantime, let's download the files one by one.

```{r, message=FALSE}
# Create the file urls and destination files names & names
base_dryad_url <- "https://datadryad.org/stash/downloads/file_stream/"
file_url_list <- paste0(base_dryad_url, c(37547:37550))
files_names <- c("monkeys.txt", "owls.txt", "oystercatchers.txt", "zurr_iena_2016.R")
files_paths_list <- paste0(c(rep("data/", 3), "scripts/"), files_names)

# If the file exists already, do not download it
ret <- mapply(\(file_url, file_path) {
  if (!file.exists(file_path)) download.file(file_url, destfile =  file_path)
}, file_url_list, files_paths_list)
```

You can take a look at the code in the scripts directory, we will be copying code from there into this document. Now, let's load the data properly before we get anything else done.

```{r}
library(here)
Owls <- read.table(here("data", "owls.txt"), 
                   header = TRUE,
                   dec = ".")

# SiblingNegotiation is too long....use shorter name:
Owls$NCalls <- Owls$SiblingNegotiation

# Let's look at it
names(Owls)
str(Owls)
head(Owls)
```

```{r}
OC <- read.table(here("data", "oystercatchers.txt"), 
                 header = TRUE,
                 dec = ".")

# Let's look at it
names(OC)
str(OC)
head(OC)
```

```{r}
Monkeys <- read.table(here("data", "monkeys.txt"), 
                      header = TRUE)

# Let's look at it
names(Monkeys)
str(Monkeys)
head(Monkeys)
```

Finally, the load the required packages. Packages `maptools` and `rgdal` are deprecated as far as I can tell, and `sp` is just not recommendable for forward compatibility in 2024. We will attempt to update the spatial code.

```{r, message=FALSE}
# Necessary packages
library(lattice)  
library(ggplot2)
library(ggmap)

# Recommended by the authors but outdated
# library(maptools) => We will use other packages
# library(sp) => We won't need it with sf
# library(rgdal) => We won't need it with sf

# Using sf
library(sf)

# Additionnal packages
library(lubridate)
library(gganimate)
```

### Step 1: State appropriate questions

The key idea is to have the salient question of the analysis in mind at the start, and formulate them properly. The example here is a study on the "vocal behavior of barn owl siblings" with a N = 28. The hypothesis is that food availability will influence "sibling negotiation", proxied by the number of calls in the nest, sampled with microphones. Half the nests get extra food (treatment: satiated) and the other half is starved (treatment: deprived ; surprisingly no control?).

The 3 covariates are *time*, *food treatment (satiated or deprived)* and *sex of parent.* The question is :

-   Does the relationship between sibling negotiation and sex of the parent differ with food treatment, and does the effect of time on sibling negotiation differ with food treatment?

Note that the question contains the 3 terms and expected interactions.

The authors warn against breaking down the questions into smaller questions such as "Is there an effect of sex of the parent?", as "A potential problem with this approach is that the residuals of one model may show patterns when compared with the covariates not used in that model, which invalidates the model assumptions."

I find this a little surprising because I was taught to build simple models before complex ones, and would naturally work with simpler single covariate models first.

### Step 2: Visualize the experimental design

This step is simple yet sometimes overlooked: visualize the sampling protocol and experimental design preferably with the help of a map.

Below we use the `sf` package to parse the data as spatial data.

```{r}
# Parse the dataframe as a sf object with the proper projection, and reproject as
# WGS 84 CRS (LAT / LONG)
Owls_sf <- st_as_sf(Owls, coords = c("Xcoord", "Ycoord"))

WGS84     <- st_crs("+proj=longlat +datum=WGS84")
projSWISS <- st_crs("+init=epsg:21781")
st_crs(Owls_sf) <- st_crs(projSWISS)

Owls_sf_tmp <- st_transform(Owls_sf, WGS84)
Owls_sf_wgs84 <- cbind(Owls_sf_tmp, st_coordinates(Owls_sf_tmp))

# Let's look at it
head(Owls_sf_wgs84)
```

The code suggests to write the points as a .kml file to open in Google Earth.

```{r}
# Write the points as kml, wich you can open in google earth
owls_kml_file <- here("data", "Owls_wgs84.kml")
if (!file.exists(owls_kml_file)) {
  st_write(Owls_sf_wgs84, 
           owls_kml_file, 
           driver = "kml", delete_dsn = TRUE)
}
```

The code relied `ggmap`, which makes an API request to Google maps. Of course, in 2024 we are required to use an API key for that. As an alternative I suggest `leaflet` which is more likely to work in the future and does not require to mess with API keys.

```{r}
library(leaflet)
leaflet(Owls_sf_wgs84) %>%
  addTiles() %>%  # Add default OpenStreetMap map tiles
  addMarkers()
```

For a more publication-oriented map with `ggmap` I would recommend using the stadiamap API.

```{r}
# I suppress the API message
glgmap <- suppressMessages(get_stadiamap(c(6.7, 46.70, 7.2, 46.96)))

p_ggmap <- ggmap(glgmap) + 
  geom_point(aes(X, Y),
             data = Owls_sf_wgs84,
             size = 4) + 
  xlab("Longitude") + ylab("Latitude") + 
  theme(text = element_text(size = 15))
p_ggmap
```

We can also do a simple `sf` + `ggplot` plot.

```{r}
p_simple <- ggplot(Owls_sf_wgs84) + 
  geom_sf(size = 4) + 
  xlab("Longitude") + ylab("Latitude") + 
  theme_light() + theme(text = element_text(size = 15))
p_simple
```

Finally, the code suggest another two plots that are not in the paper, but are useful. The first time is a plot of the time series.

```{r, fig.height=10, fig.width=8}
p_series <- ggplot() + 
  xlab("Arrival time") + ylab("Number of calls") + 
  theme(text = element_text(size = 15)) + theme_light() +
  geom_point(data = Owls_sf_wgs84,
             aes(x = ArrivalTime,
                 y = NCalls,
                 color = FoodTreatment),
             size = 2) +
  geom_line(data = Owls_sf_wgs84,
            aes(x = ArrivalTime,
                y = NCalls,
                group = FoodTreatment,
                color = FoodTreatment)) + 
  facet_wrap( ~ Nest, ncol = 4)
p_series
```

The second shows how sampling unfolds across space and time, but I find the plot a little unwieldy as it facets multiple maps.

```{r, fig.height=10, fig.width=8}
# We parse the date column as a proper date
Owls_sf_wgs84$Date_parsed <- as_date(Owls_sf_wgs84$Date, format = "%d/%m/%y")

p_ggmap_facet <- ggmap(glgmap) + 
  geom_point(aes(X, Y),
             data = Owls_sf_wgs84,
             size = 4) + 
  xlab("Longitude") + ylab("Latitude") + 
  theme(text = element_text(size = 15)) + 
  facet_wrap(~Date_parsed) 

p_ggmap_facet
```

### Step 3: Conduct data exploration

There is another even older Zurr et al. paper for the [10 steps of data exploration](https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.12577#mee312577-bib-0040). The first figure of that paper gives you the gist of the protocol.

![The 10 steps for data exploration.](https://besjournals.onlinelibrary.wiley.com/cms/asset/890dd91b-444e-430b-b41b-c7edee6653fb/mee3_1_f1.gif){fig-align="center"}

In that paper, the authors warn against "data dredging", which is when the patterns explored and discovered during data exploration influence data analysis unhealthily. Modeling and testing decisions should be determined a priori, using knowledge of the system and not a posteriori after exploration of the data. When understanding is limited, we can use exploration to help generate hyotheses, but that is fundamentally different from the principled workflow of this paper.

Then, the authors warn against certain tests and visual tools, including normality tests. The paper is a useful read and could be the next iteration of that project.

We now move from owls to oystercatchers. The study related the length of clams preyed upon and the feeding behavior of oystercatchers, accross time and space. The authors describe how the design suggests a 3-way interaction term that would in practicality only be driven by a couple of data points. The following plot shows that in location A in December, there were only two observations, both showing the same value. Note that I modified the plot to add colors to those problematic points to make the visualization clearer

```{r}
# Set a color column
OC$color_pt <- ifelse(OC$Month == "Dec" & 
                        OC$FeedingType == "Stabbers" & 
                          OC$FeedingPlot == "A", 
                      "red", "grey")

# Here is the code for Figure 3
p_OC <- ggplot() + xlab("Feeding type") + ylab("Shell length (mm)") + 
  geom_point(data = OC,
             aes(x = FeedingType, y = ShellLength),
             color = OC$color_pt,
             position = position_jitter(width = .03),
             size = 2) + 
  facet_grid(Month ~ FeedingPlot,
             scales = "fixed") + 
  theme_light() +
  theme(text = element_text(size = 15)) + 
  theme(legend.position = "none") + 
  theme(strip.text.y = element_text(size = 15,
                                    colour = "black",
                                    angle = 20),
        strip.text.x = element_text(size = 15,
                                    colour = "black",
                                    angle = 0)
  )
p_OC
```

You should spend a good amount of time on data exploration. It will also help you identify data quality issues and encoding errors.

### Step 4: Identify the dependency structure in the data

The authors warn that it is rare to find a dataset without dependency structures. They advise GLMM as a way to deal with pseudoreplicated data (relying on pooling as a way to properly model dependencies between, say, sampling locations etc..).

We now move on to the baboon dataset. I think it is the same dataset used in Statistical Rethinking for teaching multilevel modelling. The design: the researchers are interested in understanding grooming behavior between baboons. Baboon may hold different ranks within the troop (represented as a value between 0 and 1). Some 60 baboons are sampled multiple times, for an hour at a time, its grooming behavior recorded, making the receiver of the behavior another layer of dependency. This pseudoreplication structure requires a mixed-effects model with random effects (two-way nested AND crossed). The structure looks like this:

![Baboon dataset structure](https://besjournals.onlinelibrary.wiley.com/cms/asset/b2fe00b8-9877-4106-983b-2285c5dd480e/mee312577-fig-0004-m.jpg){fig-align="center"}

The authors suggest to properly report the structure. Personally I love seeing, in papers, when both a graphical and mathematical description is given.

### Step 5: Present the statistical model

### Step 6: Fit the model

### Step 7: Validate the model

### Step 8: Interpret and present the numerical output of the model

### Step 9: Create a visual representation of the model

### Step 10: Simulate from the model
